{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function # Use a function definition from future version (say 3.x from 2.7 interpreter)\n",
    "import os\n",
    "\n",
    "data_root = \"../training-data/\"\n",
    "data_lang = \"javascript\"\n",
    "data = {\n",
    "  'train': { 'file': data_root + 'train/' + data_lang + '/0.train.ctf', 'location': 0 },\n",
    "  'test': { 'file': data_root +'test/' + data_lang + '/0.test.ctf', 'location': 0 },\n",
    "  'query': { 'file': data_root + 'utils/' + 'query.wl', 'location': 1 },\n",
    "  'slots': { 'file': data_root + 'utils/' + 'slots.wl', 'location': 1 },\n",
    "  'intent': { 'file': data_root + 'utils/' + 'intent.wl', 'location': 1 },\n",
    "  'peer_words': { 'file': data_root + 'utils/querywords/' + 'peer_words.wl', 'location': 1 },    \n",
    "}\n",
    "models = {\n",
    "    'slots_model': None,\n",
    "    'intent_model': None,    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import cntk as C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setting seed\n",
    "np.random.seed(0)\n",
    "C.cntk_py.set_fixed_random_seed(1)\n",
    "C.cntk_py.force_deterministic_algorithms()\n",
    "\n",
    "query_wl = [line.rstrip('\\n') for line in open(data['query']['file'])]\n",
    "slots_wl = [line.rstrip('\\n') for line in open(data['slots']['file'])]\n",
    "intent_wl = [line.rstrip('\\n') for line in open(data['intent']['file'])]\n",
    "peer_words_wl = [line.rstrip('\\n') for line in open(data['peer_words']['file'])]\n",
    "\n",
    "# number of words in vocab, slot labels, and intent labels\n",
    "vocab_size = len(query_wl) ; num_labels = len(slots_wl) ; num_intents = len(intent_wl)    \n",
    "\n",
    "# model dimensions\n",
    "input_dim  = vocab_size\n",
    "label_dim  = num_labels\n",
    "emb_dim    = 150\n",
    "hidden_dim = 300\n",
    "\n",
    "# Create the containers for input feature (x) and the label (y)\n",
    "x = C.sequence.input_variable(vocab_size)\n",
    "y = C.sequence.input_variable(num_labels)\n",
    "\n",
    "\n",
    "def BiRecurrence(fwd, bwd):\n",
    "    F = C.layers.Recurrence(fwd)\n",
    "    G = C.layers.Recurrence(bwd, go_backwards=True)\n",
    "    x = C.placeholder()\n",
    "    apply_x = C.splice(F(x), G(x))\n",
    "    return apply_x\n",
    "\n",
    "def create_model():\n",
    "    with C.layers.default_options(initial_state=0.1):\n",
    "        return C.layers.Sequential([\n",
    "            C.layers.Embedding(emb_dim, name='embed'),\n",
    "            BiRecurrence(C.layers.LSTM(hidden_dim//2),\n",
    "                                  C.layers.LSTM(hidden_dim//2)),\n",
    "            C.layers.Dense(num_labels, name='classify')\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_reader(path, is_training):\n",
    "    return C.io.MinibatchSource(C.io.CTFDeserializer(path, C.io.StreamDefs(\n",
    "         query         = C.io.StreamDef(field='S0', shape=vocab_size,  is_sparse=True),\n",
    "         intent        = C.io.StreamDef(field='S1', shape=num_intents, is_sparse=True),  \n",
    "         slot_labels   = C.io.StreamDef(field='S2', shape=num_labels,  is_sparse=True)\n",
    "     )), randomize=is_training, max_sweeps = C.io.INFINITELY_REPEAT if is_training else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['query', 'intent', 'slot_labels'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader = create_reader(data['train']['file'], is_training=True)\n",
    "reader.streams.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_criterion_function_preferred(model, labels):\n",
    "    ce   = C.cross_entropy_with_softmax(model, labels)\n",
    "    errs = C.classification_error      (model, labels)\n",
    "    return ce, errs # (model, labels) -> (loss, error metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_test(train_reader, test_reader, model_func, stream_name=\"intent\", max_epochs=10):\n",
    "    \n",
    "    # Instantiate the model function; x is the input (feature) variable \n",
    "    model = model_func(x)\n",
    "    \n",
    "    # Instantiate the loss and error function\n",
    "    global y\n",
    "    loss, label_error = create_criterion_function_preferred(model, y)\n",
    "\n",
    "    # training config\n",
    "    epoch_size = 18000        # 18000 samples is half the dataset size \n",
    "    minibatch_size = 70\n",
    "    \n",
    "    # LR schedule over epochs \n",
    "    # In CNTK, an epoch is how often we get out of the minibatch loop to\n",
    "    # do other stuff (e.g. checkpointing, adjust learning rate, etc.)\n",
    "    # (we don't run this many epochs, but if we did, these are good values)\n",
    "    lr_per_sample = [0.003]*4+[0.0015]*24+[0.0003]\n",
    "    lr_per_minibatch = [lr * minibatch_size for lr in lr_per_sample]\n",
    "    lr_schedule = C.learning_rate_schedule(lr_per_minibatch, C.UnitType.minibatch, epoch_size)\n",
    "    \n",
    "    # Momentum schedule\n",
    "    momentum_as_time_constant = C.momentum_as_time_constant_schedule(700)\n",
    "    \n",
    "    # We use a the Adam optimizer which is known to work well on this dataset\n",
    "    # Feel free to try other optimizers from \n",
    "    # https://www.cntk.ai/pythondocs/cntk.learner.html#module-cntk.learner\n",
    "    learner = C.adam(parameters=model.parameters,\n",
    "                     lr=lr_schedule,\n",
    "                     momentum=momentum_as_time_constant,\n",
    "                     gradient_clipping_threshold_per_sample=15, \n",
    "                     gradient_clipping_with_truncation=True)\n",
    "\n",
    "    # Setup the progress updater\n",
    "    progress_printer = C.logging.ProgressPrinter(tag='Training', num_epochs=max_epochs)\n",
    "    \n",
    "    # Uncomment below for more detailed logging\n",
    "    #progress_printer = ProgressPrinter(freq=100, first=10, tag='Training', num_epochs=max_epochs) \n",
    "\n",
    "    # Instantiate the trainer\n",
    "    trainer = C.Trainer(model, (loss, label_error), learner, progress_printer)\n",
    "\n",
    "    # process minibatches and perform model training\n",
    "    C.logging.log_number_of_parameters(model)\n",
    "\n",
    "    t = 0\n",
    "    for epoch in range(max_epochs):         # loop over epochs\n",
    "        epoch_end = (epoch+1) * epoch_size\n",
    "        while t < epoch_end:                # loop over minibatches on the epoch\n",
    "            data = train_reader.next_minibatch(minibatch_size, input_map={  # fetch minibatch\n",
    "                x: train_reader.streams.query,\n",
    "                y: train_reader.streams[stream_name]\n",
    "            })\n",
    "            trainer.train_minibatch(data)               # update model with it\n",
    "            t += data[y].num_samples                    # samples so far\n",
    "        trainer.summarize_training_progress()\n",
    "    \n",
    "    while True:\n",
    "        minibatch_size = 500\n",
    "        data = test_reader.next_minibatch(minibatch_size, input_map={  # fetch minibatch\n",
    "            x: test_reader.streams.query,\n",
    "            y: test_reader.streams[stream_name]\n",
    "        })\n",
    "        if not data:                                 # until we hit the end\n",
    "            break\n",
    "        trainer.test_minibatch(data)\n",
    "    \n",
    "    trainer.summarize_test_progress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_train_test(stream_name, model_name):\n",
    "    models[model_name] = create_model()\n",
    "    train_reader = create_reader(data['train']['file'], is_training=True)\n",
    "    test_reader = create_reader(data['test']['file'], is_training=False)\n",
    "    train_test(train_reader, test_reader, models[model_name], stream_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 496218 parameters in 9 parameter tensors.\n",
      "Learning rate per minibatch: 0.21\n",
      "Finished Epoch[1 of 10]: [Training] loss = 0.135982 * 18003, metric = 3.42% * 18003 4.562s (3946.3 samples/s);\n",
      "Finished Epoch[2 of 10]: [Training] loss = 0.000101 * 18043, metric = 0.00% * 18043 3.783s (4769.5 samples/s);\n",
      "Finished Epoch[3 of 10]: [Training] loss = 0.000049 * 17968, metric = 0.00% * 17968 4.776s (3762.1 samples/s);\n",
      "Finished Epoch[4 of 10]: [Training] loss = 0.000030 * 18003, metric = 0.00% * 18003 3.190s (5643.6 samples/s);\n",
      "Learning rate per minibatch: 0.105\n",
      "Finished Epoch[5 of 10]: [Training] loss = 0.000023 * 18011, metric = 0.00% * 18011 3.715s (4848.2 samples/s);\n",
      "Finished Epoch[6 of 10]: [Training] loss = 0.000019 * 17975, metric = 0.00% * 17975 3.367s (5338.6 samples/s);\n",
      "Finished Epoch[7 of 10]: [Training] loss = 0.000016 * 18052, metric = 0.00% * 18052 3.461s (5215.8 samples/s);\n",
      "Finished Epoch[8 of 10]: [Training] loss = 0.000014 * 18002, metric = 0.00% * 18002 3.239s (5557.9 samples/s);\n",
      "Finished Epoch[9 of 10]: [Training] loss = 0.000012 * 17995, metric = 0.00% * 17995 3.482s (5168.0 samples/s);\n",
      "Finished Epoch[10 of 10]: [Training] loss = 0.000011 * 17999, metric = 0.00% * 17999 3.154s (5706.7 samples/s);\n",
      "Finished Evaluation [1]: Minibatch[1-1]: metric = 0.00% * 116;\n"
     ]
    }
   ],
   "source": [
    "do_train_test(\"slot_labels\", \"slots_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS await console . log ( Infinity ) ; / * Infinity * / EOS [67, 130, 268, 16, 483, 7, 91, 8, 20, 18, 9, 91, 9, 18, 80]\n",
      "(15, 18)\n",
      "[ 0  0  1  0  3  4  0  5 17  0  0  0  0 17  0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('BOS', 'O'),\n",
       " ('await', 'O'),\n",
       " ('console', 'global_object.name'),\n",
       " ('.', 'O'),\n",
       " ('log', 'function.name'),\n",
       " ('(', 'function.args_start'),\n",
       " ('Infinity', 'O'),\n",
       " (')', 'function.args_end'),\n",
       " (';', 'statement_end'),\n",
       " ('/', 'O'),\n",
       " ('*', 'O'),\n",
       " ('Infinity', 'O'),\n",
       " ('*', 'O'),\n",
       " ('/', 'statement_end'),\n",
       " ('EOS', 'O')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dictionaries\n",
    "query_dict = {query_wl[i]:i for i in range(len(query_wl))}\n",
    "slots_dict = {slots_wl[i]:i for i in range(len(slots_wl))}\n",
    "intent_dict = {intent_wl[i]:i for i in range(len(intent_wl))}\n",
    "undefined_word = peer_words_wl[0]\n",
    "\n",
    "# let's run a sequence through\n",
    "#seq = \"BOS let i = 10000 ; EOS\"\n",
    "seq = \"BOS var x ; if ( x = = = void 0 ) { / / these statements execute } EOS\"\n",
    "#seq = \"BOS Array . isArray ( { pin , id , i , key , name } ) ; EOS\"\n",
    "#seq = \"BOS const obj = { a : 1 , b : 20000 , c : 1 , d : 2 } ; EOS\"\n",
    "#seq = \"BOS const arr = [ a , 1 , b , 20000 , c , 1 , d , 2 ] ; EOS\"\n",
    "#seq = \"BOS const aReallyBigNumber ; EOS\"\n",
    "#seq = \"BOS var test = ' cool ' ; EOS\"\n",
    "w = [query_dict[w] if w in query_dict else query_dict[undefined_word]\n",
    "     for w in seq.split()] # convert to word indices\n",
    "print(seq, w)\n",
    "onehot = np.zeros([len(w),len(query_dict)], np.float32)\n",
    "for t in range(len(w)):\n",
    "    onehot[t,w[t]] = 1\n",
    "\n",
    "#x = C.sequence.input_variable(vocab_size)\n",
    "pred = models['slots_model'](x).eval({x:[onehot]})[0]\n",
    "print(pred.shape)\n",
    "best = np.argmax(pred,axis=1)\n",
    "print(best)\n",
    "list(zip(seq.split(),[slots_wl[s] for s in best]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = C.sequence.input_variable(vocab_size)\n",
    "y = C.input_variable(num_intents)\n",
    "\n",
    "def create_model():\n",
    "    with C.layers.default_options(initial_state=0.1):\n",
    "        return C.layers.Sequential([\n",
    "            C.layers.Embedding(emb_dim, name='embed'),\n",
    "            C.layers.Stabilizer(),\n",
    "            C.layers.Fold(C.layers.LSTM(hidden_dim), go_backwards=False),\n",
    "            C.layers.Dense(num_intents, name='classify')\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 671704 parameters in 7 parameter tensors.\n",
      "Learning rate per minibatch: 0.21\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/logging/progress_print.py\u001b[0m in \u001b[0;36mon_training_update_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m___write_progress_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maggregate_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maggregate_metric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mon_training_update_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0;31m# Override for ProgressWriter.on_training_update_end.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m___generate_progress_heartbeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "SWIG director method error.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-07f113279a75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdo_train_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"intent\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"intent_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-6fec90a9c6e2>\u001b[0m in \u001b[0;36mdo_train_test\u001b[0;34m(stream_name, model_name)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtrain_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'file'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtest_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'file'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_reader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_reader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-4a808ae5d691>\u001b[0m in \u001b[0;36mtrain_test\u001b[0;34m(train_reader, test_reader, model_func, stream_name, max_epochs)\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstreams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstream_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             })\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m               \u001b[0;31m# update model with it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m                    \u001b[0;31m# samples so far\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummarize_training_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/train/trainer.py\u001b[0m in \u001b[0;36mtrain_minibatch\u001b[0;34m(self, arguments, outputs, device, is_sweep_end)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcontains_minibatch_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                 updated = super(Trainer, self).train_minibatch_overload_for_minibatchdata(\n\u001b[0;32m--> 181\u001b[0;31m                     arguments, device)\n\u001b[0m\u001b[1;32m    182\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 updated = super(Trainer, self).train_minibatch(arguments, is_sweep_end,\n",
      "\u001b[0;32m~/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/cntk_py.py\u001b[0m in \u001b[0;36mtrain_minibatch_overload_for_minibatchdata\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   3060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3061\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_minibatch_overload_for_minibatchdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3062\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_cntk_py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainer_train_minibatch_overload_for_minibatchdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3063\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3064\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: SWIG director method error."
     ]
    }
   ],
   "source": [
    "do_train_test(\"intent\", \"intent_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's run a sequence through\n",
    "#seq = \"BOS const aReallyBigNumber = ( 100 - 1 ) * 100 ; EOS\"\n",
    "seq = \"BOS var tmp = arr [ i ] ; EOS\"\n",
    "w = [query_dict[w] if w in query_dict else query_dict[undefined_word]\n",
    "     for w in seq.split()] # convert to word indices\n",
    "onehot = np.zeros([len(w),len(query_dict)], np.float32)\n",
    "for t in range(len(w)):\n",
    "    onehot[t,w[t]] = 1\n",
    "\n",
    "pred = models[\"intent_model\"](x).eval({x:[onehot]})[0]\n",
    "best = np.argmax(pred)\n",
    "print(seq, \":\", intent_wl[best])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 857,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function # Use a function definition from future version (say 3.x from 2.7 interpreter)\n",
    "import os\n",
    "\n",
    "data_root = \"../training-data/\"\n",
    "data_lang = \"javascript\"\n",
    "data = {\n",
    "  'train': { 'file': data_root + 'train/' + data_lang + '/0.train.ctf', 'location': 0 },\n",
    "  'test': { 'file': data_root +'test/' + data_lang + '/0.test.ctf', 'location': 0 },\n",
    "  'query': { 'file': data_root + 'utils/' + 'query.wl', 'location': 1 },\n",
    "  'slots': { 'file': data_root + 'utils/' + 'slots.wl', 'location': 1 },\n",
    "  'intent': { 'file': data_root + 'utils/' + 'intent.wl', 'location': 1 },\n",
    "  'peer_words': { 'file': data_root + 'utils/querywords/' + 'peer_words.wl', 'location': 1 },    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import cntk as C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setting seed\n",
    "np.random.seed(0)\n",
    "C.cntk_py.set_fixed_random_seed(1)\n",
    "C.cntk_py.force_deterministic_algorithms()\n",
    "\n",
    "query_wl = [line.rstrip('\\n') for line in open(data['query']['file'])]\n",
    "slots_wl = [line.rstrip('\\n') for line in open(data['slots']['file'])]\n",
    "intent_wl = [line.rstrip('\\n') for line in open(data['intent']['file'])]\n",
    "peer_words_wl = [line.rstrip('\\n') for line in open(data['peer_words']['file'])]\n",
    "\n",
    "# number of words in vocab, slot labels, and intent labels\n",
    "vocab_size = len(query_wl) ; num_labels = len(slots_wl) ; num_intents = len(intent_wl)    \n",
    "\n",
    "# model dimensions\n",
    "input_dim  = vocab_size\n",
    "label_dim  = num_labels\n",
    "emb_dim    = 150\n",
    "hidden_dim = 300\n",
    "\n",
    "# Create the containers for input feature (x) and the label (y)\n",
    "x = C.sequence.input_variable(vocab_size)\n",
    "y = C.sequence.input_variable(num_labels)\n",
    "\n",
    "\n",
    "def BiRecurrence(fwd, bwd):\n",
    "    F = C.layers.Recurrence(fwd)\n",
    "    G = C.layers.Recurrence(bwd, go_backwards=True)\n",
    "    x = C.placeholder()\n",
    "    apply_x = C.splice(F(x), G(x))\n",
    "    return apply_x\n",
    "\n",
    "def create_model():\n",
    "    with C.layers.default_options(initial_state=0.1):\n",
    "        return C.layers.Sequential([\n",
    "            C.layers.Embedding(emb_dim, name='embed'),\n",
    "            BiRecurrence(C.layers.LSTM(hidden_dim//2),\n",
    "                                  C.layers.LSTM(hidden_dim//2)),\n",
    "            C.layers.Dense(num_labels, name='classify')\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-1, 150)\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# peek\n",
    "z = create_model()\n",
    "print(z.embed.E.shape)\n",
    "print(z.classify.b.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(864, 150)\n"
     ]
    }
   ],
   "source": [
    "# Pass an input and check the dimension\n",
    "z = create_model()\n",
    "print(z(x).embed.E.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_reader(path, is_training):\n",
    "    return C.io.MinibatchSource(C.io.CTFDeserializer(path, C.io.StreamDefs(\n",
    "         query         = C.io.StreamDef(field='S0', shape=vocab_size,  is_sparse=True),\n",
    "         intent        = C.io.StreamDef(field='S1', shape=num_intents, is_sparse=True),  \n",
    "         slot_labels   = C.io.StreamDef(field='S2', shape=num_labels,  is_sparse=True)\n",
    "     )), randomize=is_training, max_sweeps = C.io.INFINITELY_REPEAT if is_training else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['slot_labels', 'query', 'intent'])"
      ]
     },
     "execution_count": 863,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader = create_reader(data['train']['file'], is_training=True)\n",
    "reader.streams.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_criterion_function_preferred(model, labels):\n",
    "    ce   = C.cross_entropy_with_softmax(model, labels)\n",
    "    errs = C.classification_error      (model, labels)\n",
    "    return ce, errs # (model, labels) -> (loss, error metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_test(train_reader, test_reader, model_func, stream_name=\"intent\", max_epochs=10):\n",
    "    \n",
    "    # Instantiate the model function; x is the input (feature) variable \n",
    "    model = model_func(x)\n",
    "    \n",
    "    # Instantiate the loss and error function\n",
    "    global y\n",
    "    loss, label_error = create_criterion_function_preferred(model, y)\n",
    "\n",
    "    # training config\n",
    "    epoch_size = 18000        # 18000 samples is half the dataset size \n",
    "    minibatch_size = 70\n",
    "    \n",
    "    # LR schedule over epochs \n",
    "    # In CNTK, an epoch is how often we get out of the minibatch loop to\n",
    "    # do other stuff (e.g. checkpointing, adjust learning rate, etc.)\n",
    "    # (we don't run this many epochs, but if we did, these are good values)\n",
    "    lr_per_sample = [0.003]*4+[0.0015]*24+[0.0003]\n",
    "    lr_per_minibatch = [lr * minibatch_size for lr in lr_per_sample]\n",
    "    lr_schedule = C.learning_rate_schedule(lr_per_minibatch, C.UnitType.minibatch, epoch_size)\n",
    "    \n",
    "    # Momentum schedule\n",
    "    momentum_as_time_constant = C.momentum_as_time_constant_schedule(700)\n",
    "    \n",
    "    # We use a the Adam optimizer which is known to work well on this dataset\n",
    "    # Feel free to try other optimizers from \n",
    "    # https://www.cntk.ai/pythondocs/cntk.learner.html#module-cntk.learner\n",
    "    learner = C.adam(parameters=model.parameters,\n",
    "                     lr=lr_schedule,\n",
    "                     momentum=momentum_as_time_constant,\n",
    "                     gradient_clipping_threshold_per_sample=15, \n",
    "                     gradient_clipping_with_truncation=True)\n",
    "\n",
    "    # Setup the progress updater\n",
    "    progress_printer = C.logging.ProgressPrinter(tag='Training', num_epochs=max_epochs)\n",
    "    \n",
    "    # Uncomment below for more detailed logging\n",
    "    #progress_printer = ProgressPrinter(freq=100, first=10, tag='Training', num_epochs=max_epochs) \n",
    "\n",
    "    # Instantiate the trainer\n",
    "    trainer = C.Trainer(model, (loss, label_error), learner, progress_printer)\n",
    "\n",
    "    # process minibatches and perform model training\n",
    "    C.logging.log_number_of_parameters(model)\n",
    "\n",
    "    t = 0\n",
    "    for epoch in range(max_epochs):         # loop over epochs\n",
    "        epoch_end = (epoch+1) * epoch_size\n",
    "        while t < epoch_end:                # loop over minibatches on the epoch\n",
    "            data = train_reader.next_minibatch(minibatch_size, input_map={  # fetch minibatch\n",
    "                x: train_reader.streams.query,\n",
    "                y: train_reader.streams[stream_name]\n",
    "            })\n",
    "            trainer.train_minibatch(data)               # update model with it\n",
    "            t += data[y].num_samples                    # samples so far\n",
    "        trainer.summarize_training_progress()\n",
    "    \n",
    "    while True:\n",
    "        minibatch_size = 500\n",
    "        data = test_reader.next_minibatch(minibatch_size, input_map={  # fetch minibatch\n",
    "            x: test_reader.streams.query,\n",
    "            y: test_reader.streams[stream_name]\n",
    "        })\n",
    "        if not data:                                 # until we hit the end\n",
    "            break\n",
    "        trainer.test_minibatch(data)\n",
    "    \n",
    "    trainer.summarize_test_progress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 866,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_train_test(stream_name):\n",
    "    global z\n",
    "    z = create_model()\n",
    "    train_reader = create_reader(data['train']['file'], is_training=True)\n",
    "    test_reader = create_reader(data['test']['file'], is_training=False)\n",
    "    train_test(train_reader, test_reader, z, stream_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#do_train_test(\"slot_labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS const arr = [ a , 1 , b , 20000 , c , 1 , d , 2 ] ; EOS [67, 269, 130, 34, 178, 130, 13, 130, 13, 130, 13, 130, 13, 130, 13, 130, 13, 130, 13, 130, 189, 20, 80]\n",
      "(23, 18)\n",
      "[ 7  7  7  7 11 13 16  3 16  3 11  3 16  3 16  3 16 13 16 13 16 10 10]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('BOS', 'declaration.variable'),\n",
       " ('const', 'declaration.variable'),\n",
       " ('arr', 'declaration.variable'),\n",
       " ('=', 'declaration.variable'),\n",
       " ('[', 'literal.array.start'),\n",
       " ('a', 'literal.object.start'),\n",
       " (',', 'literal.object.delimiter'),\n",
       " ('1', 'function.name'),\n",
       " (',', 'literal.object.delimiter'),\n",
       " ('b', 'function.name'),\n",
       " (',', 'literal.array.start'),\n",
       " ('20000', 'function.name'),\n",
       " (',', 'literal.object.delimiter'),\n",
       " ('c', 'function.name'),\n",
       " (',', 'literal.object.delimiter'),\n",
       " ('1', 'function.name'),\n",
       " (',', 'literal.object.delimiter'),\n",
       " ('d', 'literal.object.start'),\n",
       " (',', 'literal.object.delimiter'),\n",
       " ('2', 'literal.object.start'),\n",
       " (']', 'literal.object.delimiter'),\n",
       " (';', 'literal.string.end'),\n",
       " ('EOS', 'literal.string.end')]"
      ]
     },
     "execution_count": 868,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dictionaries\n",
    "query_dict = {query_wl[i]:i for i in range(len(query_wl))}\n",
    "slots_dict = {slots_wl[i]:i for i in range(len(slots_wl))}\n",
    "intent_dict = {intent_wl[i]:i for i in range(len(intent_wl))}\n",
    "undefined_word = peer_words_wl[0]\n",
    "\n",
    "# let's run a sequence through\n",
    "#seq = \"BOS let i = 10000 ; EOS\"\n",
    "#seq = \"BOS isNaN ( 100 ) ; EOS\"\n",
    "#seq = \"BOS Array . isArray ( { pin , id , i , key , name } ) ; EOS\"\n",
    "#seq = \"BOS const obj = { a : 1 , b : 20000 , c : 1 , d : 2 } ; EOS\"\n",
    "seq = \"BOS const arr = [ a , 1 , b , 20000 , c , 1 , d , 2 ] ; EOS\"\n",
    "w = [query_dict[w] if w in query_dict else query_dict[undefined_word]\n",
    "     for w in seq.split()] # convert to word indices\n",
    "print(seq, w)\n",
    "onehot = np.zeros([len(w),len(query_dict)], np.float32)\n",
    "for t in range(len(w)):\n",
    "    onehot[t,w[t]] = 1\n",
    "\n",
    "#x = C.sequence.input_variable(vocab_size)\n",
    "pred = z(x).eval({x:[onehot]})[0]\n",
    "print(pred.shape)\n",
    "best = np.argmax(pred,axis=1)\n",
    "print(best)\n",
    "list(zip(seq.split(),[slots_wl[s] for s in best]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = C.sequence.input_variable(vocab_size)\n",
    "y = C.input_variable(num_intents)\n",
    "\n",
    "def create_model():\n",
    "    with C.layers.default_options(initial_state=0.1):\n",
    "        return C.layers.Sequential([\n",
    "            C.layers.Embedding(emb_dim, name='embed'),\n",
    "            C.layers.Stabilizer(),\n",
    "            C.layers.Fold(C.layers.LSTM(hidden_dim), go_backwards=False),\n",
    "            C.layers.Dense(num_intents, name='classify')\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 671704 parameters in 7 parameter tensors.\n",
      "Learning rate per minibatch: 0.21\n",
      "Finished Epoch[1 of 10]: [Training] loss = 0.009904 * 18004, metric = 0.33% * 18004 69.166s (260.3 samples/s);\n",
      "Finished Epoch[2 of 10]: [Training] loss = 0.000000 * 18001, metric = 0.00% * 18001 77.480s (232.3 samples/s);\n",
      "Finished Epoch[3 of 10]: [Training] loss = 0.000000 * 17999, metric = 0.00% * 17999 74.795s (240.6 samples/s);\n",
      "Finished Epoch[4 of 10]: [Training] loss = 0.000000 * 17999, metric = 0.00% * 17999 77.039s (233.6 samples/s);\n",
      "Learning rate per minibatch: 0.105\n",
      "Finished Epoch[5 of 10]: [Training] loss = 0.000000 * 17999, metric = 0.00% * 17999 75.778s (237.5 samples/s);\n",
      "Finished Epoch[6 of 10]: [Training] loss = 0.000000 * 18001, metric = 0.00% * 18001 72.644s (247.8 samples/s);\n",
      "Finished Epoch[7 of 10]: [Training] loss = 0.000000 * 18001, metric = 0.00% * 18001 75.556s (238.2 samples/s);\n",
      "Finished Epoch[8 of 10]: [Training] loss = 0.000000 * 18000, metric = 0.00% * 18000 87.374s (206.0 samples/s);\n",
      "Finished Epoch[9 of 10]: [Training] loss = 0.000000 * 17996, metric = 0.00% * 17996 77.171s (233.2 samples/s);\n",
      "Finished Epoch[10 of 10]: [Training] loss = 0.000000 * 18004, metric = 0.00% * 18004 84.341s (213.5 samples/s);\n",
      "Finished Evaluation [1]: Minibatch[1-1]: metric = 0.00% * 6;\n"
     ]
    }
   ],
   "source": [
    "do_train_test(\"intent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "BOS isNaN ( 100 ) ; EOS : invocation\n"
     ]
    }
   ],
   "source": [
    "# let's run a sequence through\n",
    "seq = \"BOS isNaN ( 100 ) ; EOS\"\n",
    "w = [query_dict[w] if w in query_dict else query_dict[undefined_word]\n",
    "     for w in seq.split()] # convert to word indices\n",
    "onehot = np.zeros([len(w),len(query_dict)], np.float32)\n",
    "for t in range(len(w)):\n",
    "    onehot[t,w[t]] = 1\n",
    "\n",
    "pred = z(x).eval({x:[onehot]})[0]\n",
    "best = np.argmax(pred)\n",
    "print(best)\n",
    "print(seq, \":\", intent_wl[best])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

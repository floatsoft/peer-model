{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function # Use a function definition from future version (say 3.x from 2.7 interpreter)\n",
    "import os\n",
    "\n",
    "data_root = \"../training-data/\"\n",
    "data_lang = \"javascript\"\n",
    "data = {\n",
    "  'train': { 'file': data_root + 'train/' + data_lang + '/0.train.ctf', 'location': 0 },\n",
    "  'test': { 'file': data_root +'test/' + data_lang + '/0.test.ctf', 'location': 0 },\n",
    "  'query': { 'file': data_root + 'utils/' + 'query.wl', 'location': 1 },\n",
    "  'slots': { 'file': data_root + 'utils/' + 'slots.wl', 'location': 1 },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import cntk as C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setting seed\n",
    "np.random.seed(0)\n",
    "C.cntk_py.set_fixed_random_seed(1)\n",
    "C.cntk_py.force_deterministic_algorithms()\n",
    "\n",
    "query_wl = [line.rstrip('\\n') for line in open(data['query']['file'])]\n",
    "slots_wl = [line.rstrip('\\n') for line in open(data['slots']['file'])]\n",
    "\n",
    "# number of words in vocab, slot labels, and intent labels\n",
    "vocab_size = len(query_wl) ; num_labels = len(slots_wl) ; num_intents = 1    \n",
    "\n",
    "# model dimensions\n",
    "input_dim  = vocab_size\n",
    "label_dim  = num_labels\n",
    "emb_dim    = 150\n",
    "hidden_dim = 300\n",
    "\n",
    "# Create the containers for input feature (x) and the label (y)\n",
    "x = C.sequence.input_variable(vocab_size)\n",
    "y = C.sequence.input_variable(num_labels)\n",
    "\n",
    "def create_model():\n",
    "    with C.layers.default_options(initial_state=0.1):\n",
    "        return C.layers.Sequential([\n",
    "            C.layers.Embedding(emb_dim, name='embed'),\n",
    "            C.layers.Recurrence(C.layers.LSTM(hidden_dim), go_backwards=False),\n",
    "            C.layers.Dense(num_labels, name='classify')\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-1, 150)\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# peek\n",
    "z = create_model()\n",
    "print(z.embed.E.shape)\n",
    "print(z.classify.b.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 150)\n"
     ]
    }
   ],
   "source": [
    "# Pass an input and check the dimension\n",
    "z = create_model()\n",
    "print(z(x).embed.E.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_reader(path, is_training):\n",
    "    return C.io.MinibatchSource(C.io.CTFDeserializer(path, C.io.StreamDefs(\n",
    "         query         = C.io.StreamDef(field='S0', shape=vocab_size,  is_sparse=True),\n",
    "         intent_unused = C.io.StreamDef(field='S1', shape=num_intents, is_sparse=True),  \n",
    "         slot_labels   = C.io.StreamDef(field='S2', shape=num_labels,  is_sparse=True)\n",
    "     )), randomize=is_training, max_sweeps = C.io.INFINITELY_REPEAT if is_training else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['slot_labels', 'query', 'intent_unused'])"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader = create_reader(data['train']['file'], is_training=True)\n",
    "reader.streams.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_criterion_function_preferred(model, labels):\n",
    "    ce   = C.cross_entropy_with_softmax(model, labels)\n",
    "    errs = C.classification_error      (model, labels)\n",
    "    return ce, errs # (model, labels) -> (loss, error metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_test(train_reader, test_reader, model_func, max_epochs=10):\n",
    "    \n",
    "    # Instantiate the model function; x is the input (feature) variable \n",
    "    model = model_func(x)\n",
    "    \n",
    "    # Instantiate the loss and error function\n",
    "    loss, label_error = create_criterion_function_preferred(model, y)\n",
    "\n",
    "    # training config\n",
    "    epoch_size = 18000        # 18000 samples is half the dataset size \n",
    "    minibatch_size = 70\n",
    "    \n",
    "    # LR schedule over epochs \n",
    "    # In CNTK, an epoch is how often we get out of the minibatch loop to\n",
    "    # do other stuff (e.g. checkpointing, adjust learning rate, etc.)\n",
    "    # (we don't run this many epochs, but if we did, these are good values)\n",
    "    lr_per_sample = [0.003]*4+[0.0015]*24+[0.0003]\n",
    "    lr_per_minibatch = [lr * minibatch_size for lr in lr_per_sample]\n",
    "    lr_schedule = C.learning_rate_schedule(lr_per_minibatch, C.UnitType.minibatch, epoch_size)\n",
    "    \n",
    "    # Momentum schedule\n",
    "    momentum_as_time_constant = C.momentum_as_time_constant_schedule(700)\n",
    "    \n",
    "    # We use a the Adam optimizer which is known to work well on this dataset\n",
    "    # Feel free to try other optimizers from \n",
    "    # https://www.cntk.ai/pythondocs/cntk.learner.html#module-cntk.learner\n",
    "    learner = C.adam(parameters=model.parameters,\n",
    "                     lr=lr_schedule,\n",
    "                     momentum=momentum_as_time_constant,\n",
    "                     gradient_clipping_threshold_per_sample=15, \n",
    "                     gradient_clipping_with_truncation=True)\n",
    "\n",
    "    # Setup the progress updater\n",
    "    progress_printer = C.logging.ProgressPrinter(tag='Training', num_epochs=max_epochs)\n",
    "    \n",
    "    # Uncomment below for more detailed logging\n",
    "    #progress_printer = ProgressPrinter(freq=100, first=10, tag='Training', num_epochs=max_epochs) \n",
    "\n",
    "    # Instantiate the trainer\n",
    "    trainer = C.Trainer(model, (loss, label_error), learner, progress_printer)\n",
    "\n",
    "    # process minibatches and perform model training\n",
    "    C.logging.log_number_of_parameters(model)\n",
    "\n",
    "    t = 0\n",
    "    for epoch in range(max_epochs):         # loop over epochs\n",
    "        epoch_end = (epoch+1) * epoch_size\n",
    "        while t < epoch_end:                # loop over minibatches on the epoch\n",
    "            data = train_reader.next_minibatch(minibatch_size, input_map={  # fetch minibatch\n",
    "                x: train_reader.streams.query,\n",
    "                y: train_reader.streams.slot_labels\n",
    "            })\n",
    "            trainer.train_minibatch(data)               # update model with it\n",
    "            t += data[y].num_samples                    # samples so far\n",
    "        trainer.summarize_training_progress()\n",
    "    \n",
    "    while True:\n",
    "        minibatch_size = 500\n",
    "        data = test_reader.next_minibatch(minibatch_size, input_map={  # fetch minibatch\n",
    "            x: test_reader.streams.query,\n",
    "            y: test_reader.streams.slot_labels\n",
    "        })\n",
    "        if not data:                                 # until we hit the end\n",
    "            break\n",
    "        trainer.test_minibatch(data)\n",
    "    \n",
    "    trainer.summarize_test_progress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_train_test():\n",
    "    global z\n",
    "    z = create_model()\n",
    "    train_reader = create_reader(data['train']['file'], is_training=True)\n",
    "    test_reader = create_reader(data['test']['file'], is_training=False)\n",
    "    train_test(train_reader, test_reader, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 548408 parameters in 6 parameter tensors.\n",
      "Learning rate per minibatch: 0.21\n",
      "Finished Epoch[1 of 10]: [Training] loss = 0.052017 * 18042, metric = 1.59% * 18042 3.368s (5356.9 samples/s);\n",
      "Finished Epoch[2 of 10]: [Training] loss = 0.000022 * 17976, metric = 0.00% * 17976 3.547s (5067.9 samples/s);\n",
      "Finished Epoch[3 of 10]: [Training] loss = 0.000012 * 18003, metric = 0.00% * 18003 3.397s (5299.7 samples/s);\n",
      "Finished Epoch[4 of 10]: [Training] loss = 0.000008 * 17991, metric = 0.00% * 17991 3.290s (5468.4 samples/s);\n",
      "Learning rate per minibatch: 0.105\n",
      "Finished Epoch[5 of 10]: [Training] loss = 0.000006 * 18048, metric = 0.00% * 18048 3.078s (5863.5 samples/s);\n",
      "Finished Epoch[6 of 10]: [Training] loss = 0.000005 * 17970, metric = 0.00% * 17970 4.009s (4482.4 samples/s);\n",
      "Finished Epoch[7 of 10]: [Training] loss = 0.000005 * 17988, metric = 0.00% * 17988 3.253s (5529.7 samples/s);\n",
      "Finished Epoch[8 of 10]: [Training] loss = 0.000004 * 18024, metric = 0.00% * 18024 3.130s (5758.5 samples/s);\n",
      "Finished Epoch[9 of 10]: [Training] loss = 0.000004 * 17958, metric = 0.00% * 17958 4.366s (4113.1 samples/s);\n",
      "Finished Epoch[10 of 10]: [Training] loss = 0.000003 * 18048, metric = 0.00% * 18048 3.376s (5346.0 samples/s);\n",
      "Finished Evaluation [1]: Minibatch[1-1]: metric = 1.59% * 63;\n"
     ]
    }
   ],
   "source": [
    "do_train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'info': 23, 'BOS': 1, '[': 7, 'keys': 29, 'warn': 15, '{': 3, '(': 5, 'values': 31, 'error': 16, 'trace': 27, 'groupEnd': 22, '\"': 12, 'count': 17, 'console': 13, 'Object': 28, 'unknown': 0, 'log': 14, 'timeEnd': 26, 'group': 20, 'table': 24, \"'\": 11, ')': 6, ']': 8, 'time': 25, '}': 4, 'assert': 18, '.': 10, 'EOS': 2, ';': 9, 'entries': 30, 'groupCollapsed': 21, 'clear': 19}\n",
      "[1, 28, 10, 30, 5, 0, 6, 9, 2]\n",
      "(9, 8)\n",
      "[7 0 7 1 2 7 3 6 7]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('BOS', 'O'),\n",
       " ('Object', 'class'),\n",
       " ('.', 'O'),\n",
       " ('entries', 'method.name'),\n",
       " ('(', 'method.args_start'),\n",
       " ('unknown', 'O'),\n",
       " (')', 'method.args_end'),\n",
       " (';', 'statement_end'),\n",
       " ('EOS', 'O')]"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dictionaries\n",
    "query_dict = {query_wl[i]:i for i in range(len(query_wl))}\n",
    "slots_dict = {slots_wl[i]:i for i in range(len(slots_wl))}\n",
    "print(query_dict)\n",
    "\n",
    "# let's run a sequence through\n",
    "seq = \"BOS Object . entries ( unknown ) ; EOS\"\n",
    "w = [query_dict[w] for w in seq.split()] # convert to word indices\n",
    "print(w)\n",
    "onehot = np.zeros([len(w),len(query_dict)], np.float32)\n",
    "for t in range(len(w)):\n",
    "    onehot[t,w[t]] = 1\n",
    "\n",
    "#x = C.sequence.input_variable(vocab_size)\n",
    "pred = z(x).eval({x:[onehot]})[0]\n",
    "print(pred.shape)\n",
    "best = np.argmax(pred,axis=1)\n",
    "print(best)\n",
    "list(zip(seq.split(),[slots_wl[s] for s in best]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

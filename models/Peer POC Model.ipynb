{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function # Use a function definition from future version (say 3.x from 2.7 interpreter)\n",
    "import os\n",
    "\n",
    "data_root = \"../training-data/\"\n",
    "data_lang = \"javascript\"\n",
    "data = {\n",
    "  'train': { 'file': data_root + 'train/' + data_lang + '/0.train.ctf', 'location': 0 },\n",
    "  'test': { 'file': data_root +'test/' + data_lang + '/0.test.ctf', 'location': 0 },\n",
    "  'query': { 'file': data_root + 'utils/' + 'query.wl', 'location': 1 },\n",
    "  'slots': { 'file': data_root + 'utils/' + 'slots.wl', 'location': 1 },\n",
    "  'intent': { 'file': data_root + 'utils/' + 'intent.wl', 'location': 1 },\n",
    "  'peer_words': { 'file': data_root + 'utils/querywords/' + 'peer_words.wl', 'location': 1 },    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import cntk as C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting seed\n",
    "np.random.seed(0)\n",
    "C.cntk_py.set_fixed_random_seed(1)\n",
    "C.cntk_py.force_deterministic_algorithms()\n",
    "\n",
    "query_wl = [line.rstrip('\\n') for line in open(data['query']['file'])]\n",
    "slots_wl = [line.rstrip('\\n') for line in open(data['slots']['file'])]\n",
    "intent_wl = [line.rstrip('\\n') for line in open(data['intent']['file'])]\n",
    "peer_words_wl = [line.rstrip('\\n') for line in open(data['peer_words']['file'])]\n",
    "\n",
    "# number of words in vocab, slot labels, and intent labels\n",
    "vocab_size = len(query_wl) ; num_labels = len(slots_wl) ; num_intents = len(intent_wl)    \n",
    "\n",
    "# model dimensions\n",
    "input_dim  = vocab_size\n",
    "label_dim  = num_labels\n",
    "emb_dim    = 150\n",
    "hidden_dim = 300\n",
    "\n",
    "# Create the containers for input feature (x) and the label (y)\n",
    "x = C.sequence.input_variable(vocab_size)\n",
    "y = C.sequence.input_variable(num_labels)\n",
    "\n",
    "def create_model():\n",
    "    with C.layers.default_options(initial_state=0.1):\n",
    "        return C.layers.Sequential([\n",
    "            C.layers.Embedding(emb_dim, name='embed'),\n",
    "            C.layers.Recurrence(C.layers.LSTM(hidden_dim), go_backwards=False),\n",
    "            C.layers.Dense(num_labels, name='classify')\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-1, 150)\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# peek\n",
    "z = create_model()\n",
    "print(z.embed.E.shape)\n",
    "print(z.classify.b.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(864, 150)\n"
     ]
    }
   ],
   "source": [
    "# Pass an input and check the dimension\n",
    "z = create_model()\n",
    "print(z(x).embed.E.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_reader(path, is_training):\n",
    "    return C.io.MinibatchSource(C.io.CTFDeserializer(path, C.io.StreamDefs(\n",
    "         query         = C.io.StreamDef(field='S0', shape=vocab_size,  is_sparse=True),\n",
    "         intent_unused = C.io.StreamDef(field='S1', shape=num_intents, is_sparse=True),  \n",
    "         slot_labels   = C.io.StreamDef(field='S2', shape=num_labels,  is_sparse=True)\n",
    "     )), randomize=is_training, max_sweeps = C.io.INFINITELY_REPEAT if is_training else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['slot_labels', 'query', 'intent_unused'])"
      ]
     },
     "execution_count": 584,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader = create_reader(data['train']['file'], is_training=True)\n",
    "reader.streams.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_criterion_function_preferred(model, labels):\n",
    "    ce   = C.cross_entropy_with_softmax(model, labels)\n",
    "    errs = C.classification_error      (model, labels)\n",
    "    return ce, errs # (model, labels) -> (loss, error metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_test(train_reader, test_reader, model_func, max_epochs=10):\n",
    "    \n",
    "    # Instantiate the model function; x is the input (feature) variable \n",
    "    model = model_func(x)\n",
    "    \n",
    "    # Instantiate the loss and error function\n",
    "    loss, label_error = create_criterion_function_preferred(model, y)\n",
    "\n",
    "    # training config\n",
    "    epoch_size = 18000        # 18000 samples is half the dataset size \n",
    "    minibatch_size = 70\n",
    "    \n",
    "    # LR schedule over epochs \n",
    "    # In CNTK, an epoch is how often we get out of the minibatch loop to\n",
    "    # do other stuff (e.g. checkpointing, adjust learning rate, etc.)\n",
    "    # (we don't run this many epochs, but if we did, these are good values)\n",
    "    lr_per_sample = [0.003]*4+[0.0015]*24+[0.0003]\n",
    "    lr_per_minibatch = [lr * minibatch_size for lr in lr_per_sample]\n",
    "    lr_schedule = C.learning_rate_schedule(lr_per_minibatch, C.UnitType.minibatch, epoch_size)\n",
    "    \n",
    "    # Momentum schedule\n",
    "    momentum_as_time_constant = C.momentum_as_time_constant_schedule(700)\n",
    "    \n",
    "    # We use a the Adam optimizer which is known to work well on this dataset\n",
    "    # Feel free to try other optimizers from \n",
    "    # https://www.cntk.ai/pythondocs/cntk.learner.html#module-cntk.learner\n",
    "    learner = C.adam(parameters=model.parameters,\n",
    "                     lr=lr_schedule,\n",
    "                     momentum=momentum_as_time_constant,\n",
    "                     gradient_clipping_threshold_per_sample=15, \n",
    "                     gradient_clipping_with_truncation=True)\n",
    "\n",
    "    # Setup the progress updater\n",
    "    progress_printer = C.logging.ProgressPrinter(tag='Training', num_epochs=max_epochs)\n",
    "    \n",
    "    # Uncomment below for more detailed logging\n",
    "    #progress_printer = ProgressPrinter(freq=100, first=10, tag='Training', num_epochs=max_epochs) \n",
    "\n",
    "    # Instantiate the trainer\n",
    "    trainer = C.Trainer(model, (loss, label_error), learner, progress_printer)\n",
    "\n",
    "    # process minibatches and perform model training\n",
    "    C.logging.log_number_of_parameters(model)\n",
    "\n",
    "    t = 0\n",
    "    for epoch in range(max_epochs):         # loop over epochs\n",
    "        epoch_end = (epoch+1) * epoch_size\n",
    "        while t < epoch_end:                # loop over minibatches on the epoch\n",
    "            data = train_reader.next_minibatch(minibatch_size, input_map={  # fetch minibatch\n",
    "                x: train_reader.streams.query,\n",
    "                y: train_reader.streams.slot_labels\n",
    "            })\n",
    "            trainer.train_minibatch(data)               # update model with it\n",
    "            t += data[y].num_samples                    # samples so far\n",
    "        trainer.summarize_training_progress()\n",
    "    \n",
    "    while True:\n",
    "        minibatch_size = 500\n",
    "        data = test_reader.next_minibatch(minibatch_size, input_map={  # fetch minibatch\n",
    "            x: test_reader.streams.query,\n",
    "            y: test_reader.streams.slot_labels\n",
    "        })\n",
    "        if not data:                                 # until we hit the end\n",
    "            break\n",
    "        trainer.test_minibatch(data)\n",
    "    \n",
    "    trainer.summarize_test_progress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_train_test():\n",
    "    global z\n",
    "    z = create_model()\n",
    "    train_reader = create_reader(data['train']['file'], is_training=True)\n",
    "    test_reader = create_reader(data['test']['file'], is_training=False)\n",
    "    train_test(train_reader, test_reader, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 675917 parameters in 6 parameter tensors.\n",
      "Learning rate per minibatch: 0.21\n",
      "Finished Epoch[1 of 10]: [Training] loss = 0.168938 * 18021, metric = 3.73% * 18021 1.964s (9175.7 samples/s);\n",
      "Finished Epoch[2 of 10]: [Training] loss = 0.000166 * 18035, metric = 0.00% * 18035 1.810s (9964.1 samples/s);\n",
      "Finished Epoch[3 of 10]: [Training] loss = 0.000075 * 18011, metric = 0.00% * 18011 1.815s (9923.4 samples/s);\n",
      "Finished Epoch[4 of 10]: [Training] loss = 0.000045 * 17992, metric = 0.00% * 17992 1.808s (9951.3 samples/s);\n",
      "Learning rate per minibatch: 0.105\n",
      "Finished Epoch[5 of 10]: [Training] loss = 0.000033 * 17972, metric = 0.00% * 17972 1.825s (9847.7 samples/s);\n",
      "Finished Epoch[6 of 10]: [Training] loss = 0.000027 * 18027, metric = 0.00% * 18027 1.936s (9311.5 samples/s);\n",
      "Finished Epoch[7 of 10]: [Training] loss = 0.000023 * 17959, metric = 0.00% * 17959 1.961s (9158.1 samples/s);\n",
      "Finished Epoch[8 of 10]: [Training] loss = 0.000020 * 18040, metric = 0.00% * 18040 2.407s (7494.8 samples/s);\n",
      "Finished Epoch[9 of 10]: [Training] loss = 0.000017 * 17948, metric = 0.00% * 17948 1.958s (9166.5 samples/s);\n",
      "Finished Epoch[10 of 10]: [Training] loss = 0.000015 * 18048, metric = 0.00% * 18048 1.987s (9083.0 samples/s);\n",
      "Finished Evaluation [1]: Minibatch[1-1]: metric = 0.00% * 63;\n"
     ]
    }
   ],
   "source": [
    "do_train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS Array . isArray ( [ 1 , 2 , 3 , 4 ] ) ; EOS [67, 40, 16, 447, 7, 178, 130, 13, 130, 13, 130, 13, 130, 189, 8, 20, 80]\n",
      "(17, 17)\n",
      "[ 0  1  0  3  4 11  0 15  0 15  0 12  0 12  5 16  0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('BOS', 'O'),\n",
       " ('Array', 'object.name'),\n",
       " ('.', 'O'),\n",
       " ('isArray', 'function.name'),\n",
       " ('(', 'function.args_start'),\n",
       " ('[', 'literal.array.start'),\n",
       " ('1', 'O'),\n",
       " (',', 'separator'),\n",
       " ('2', 'O'),\n",
       " (',', 'separator'),\n",
       " ('3', 'O'),\n",
       " (',', 'literal.array.end'),\n",
       " ('4', 'O'),\n",
       " (']', 'literal.array.end'),\n",
       " (')', 'function.args_end'),\n",
       " (';', 'statement_end'),\n",
       " ('EOS', 'O')]"
      ]
     },
     "execution_count": 590,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dictionaries\n",
    "query_dict = {query_wl[i]:i for i in range(len(query_wl))}\n",
    "slots_dict = {slots_wl[i]:i for i in range(len(slots_wl))}\n",
    "undefined_word = peer_words_wl[0]\n",
    "\n",
    "# let's run a sequence through\n",
    "#seq = \"BOS let i = 10000 ; EOS\"\n",
    "#seq = \"BOS isNaN ( 100 ) ; EOS\"\n",
    "seq = \"BOS Array . isArray ( [ 1 , 2 , 3 , 4 ] ) ; EOS\"\n",
    "#seq = \"BOS let arr = [ a , 20000 , 1 , 2 ] ; EOS\"\n",
    "w = [query_dict[w] if w in query_dict else query_dict[undefined_word]\n",
    "     for w in seq.split()] # convert to word indices\n",
    "print(seq, w)\n",
    "onehot = np.zeros([len(w),len(query_dict)], np.float32)\n",
    "for t in range(len(w)):\n",
    "    onehot[t,w[t]] = 1\n",
    "\n",
    "#x = C.sequence.input_variable(vocab_size)\n",
    "pred = z(x).eval({x:[onehot]})[0]\n",
    "print(pred.shape)\n",
    "best = np.argmax(pred,axis=1)\n",
    "print(best)\n",
    "list(zip(seq.split(),[slots_wl[s] for s in best]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function # Use a function definition from future version (say 3.x from 2.7 interpreter)\n",
    "import os\n",
    "\n",
    "data_root = \"../training-data/\"\n",
    "data_lang = \"javascript\"\n",
    "data = {\n",
    "  'train': { 'file': data_root + 'train/' + data_lang + '/0.train.ctf', 'location': 0 },\n",
    "  'test': { 'file': data_root +'test/' + data_lang + '/0.test.ctf', 'location': 0 },\n",
    "  'query': { 'file': data_root + 'utils/' + 'query.wl', 'location': 1 },\n",
    "  'slots': { 'file': data_root + 'utils/' + 'slots.wl', 'location': 1 },\n",
    "  'intent': { 'file': data_root + 'utils/' + 'intent.wl', 'location': 1 },\n",
    "  'peer_words': { 'file': data_root + 'utils/querywords/' + 'peer_words.wl', 'location': 1 },    \n",
    "}\n",
    "models = {\n",
    "    'slots_model': None,\n",
    "    'intent_model': None,    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/cntk_py_init.py:90: UserWarning: \n",
      "\n",
      "################################################ Missing optional dependency (    MKL     ) ################################################\n",
      "   CNTK may crash if the component that depends on those dependencies is loaded.\n",
      "   Visit https://docs.microsoft.com/en-us/cognitive-toolkit/Setup-Linux-Python#mkl for more information.\n",
      "############################################################################################################################################\n",
      "\n",
      "  warnings.warn(WARNING_MSG % ('    MKL     ', 'https://docs.microsoft.com/en-us/cognitive-toolkit/Setup-Linux-Python#mkl'))\n",
      "/root/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/cntk_py_init.py:98: UserWarning: \n",
      "\n",
      "################################################ Missing optional dependency (GPU-Specific) ################################################\n",
      "   CNTK may crash if the component that depends on those dependencies is loaded.\n",
      "   Visit https://docs.microsoft.com/en-us/cognitive-toolkit/Setup-Linux-Python#optional-gpu-specific-packages for more information.\n",
      "############################################################################################################################################\n",
      "If you intend to use CNTK without GPU support, you can ignore the (likely) GPU-specific warning!\n",
      "############################################################################################################################################\n",
      "\n",
      "  warnings.warn(WARNING_MSG_GPU_ONLY % ('GPU-Specific', 'https://docs.microsoft.com/en-us/cognitive-toolkit/Setup-Linux-Python#optional-gpu-specific-packages'))\n",
      "/root/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/cntk_py_init.py:102: UserWarning: \n",
      "\n",
      "################################################ Missing optional dependency (   OpenCV   ) ################################################\n",
      "   CNTK may crash if the component that depends on those dependencies is loaded.\n",
      "   Visit https://docs.microsoft.com/en-us/cognitive-toolkit/Setup-Linux-Python#optional-opencv for more information.\n",
      "############################################################################################################################################\n",
      "\n",
      "  warnings.warn(WARNING_MSG % ('   OpenCV   ', 'https://docs.microsoft.com/en-us/cognitive-toolkit/Setup-Linux-Python#optional-opencv'))\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import cntk as C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setting seed\n",
    "np.random.seed(0)\n",
    "C.cntk_py.set_fixed_random_seed(1)\n",
    "C.cntk_py.force_deterministic_algorithms()\n",
    "\n",
    "query_wl = [line.rstrip('\\n') for line in open(data['query']['file'])]\n",
    "slots_wl = [line.rstrip('\\n') for line in open(data['slots']['file'])]\n",
    "intent_wl = [line.rstrip('\\n') for line in open(data['intent']['file'])]\n",
    "peer_words_wl = [line.rstrip('\\n') for line in open(data['peer_words']['file'])]\n",
    "\n",
    "# number of words in vocab, slot labels, and intent labels\n",
    "vocab_size = len(query_wl) ; num_labels = len(slots_wl) ; num_intents = len(intent_wl)    \n",
    "\n",
    "# model dimensions\n",
    "input_dim  = vocab_size\n",
    "label_dim  = num_labels\n",
    "emb_dim    = 150\n",
    "hidden_dim = 300\n",
    "\n",
    "# Create the containers for input feature (x) and the label (y)\n",
    "x = C.sequence.input_variable(vocab_size)\n",
    "y = C.sequence.input_variable(num_labels)\n",
    "\n",
    "\n",
    "def BiRecurrence(fwd, bwd):\n",
    "    F = C.layers.Recurrence(fwd)\n",
    "    G = C.layers.Recurrence(bwd, go_backwards=True)\n",
    "    x = C.placeholder()\n",
    "    apply_x = C.splice(F(x), G(x))\n",
    "    return apply_x\n",
    "\n",
    "def create_model():\n",
    "    with C.layers.default_options(initial_state=0.1):\n",
    "        return C.layers.Sequential([\n",
    "            C.layers.Embedding(emb_dim, name='embed'),\n",
    "            BiRecurrence(C.layers.LSTM(hidden_dim//2),\n",
    "                                  C.layers.LSTM(hidden_dim//2)),\n",
    "            C.layers.Dense(num_labels, name='classify')\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_reader(path, is_training):\n",
    "    return C.io.MinibatchSource(C.io.CTFDeserializer(path, C.io.StreamDefs(\n",
    "         query         = C.io.StreamDef(field='S0', shape=vocab_size,  is_sparse=True),\n",
    "         intent        = C.io.StreamDef(field='S1', shape=num_intents, is_sparse=True),  \n",
    "         slot_labels   = C.io.StreamDef(field='S2', shape=num_labels,  is_sparse=True)\n",
    "     )), randomize=is_training, max_sweeps = C.io.INFINITELY_REPEAT if is_training else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['slot_labels', 'query', 'intent'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader = create_reader(data['train']['file'], is_training=True)\n",
    "reader.streams.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_criterion_function_preferred(model, labels):\n",
    "    ce   = C.cross_entropy_with_softmax(model, labels)\n",
    "    errs = C.classification_error      (model, labels)\n",
    "    return ce, errs # (model, labels) -> (loss, error metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_test(train_reader, test_reader, model_func, stream_name=\"intent\", max_epochs=10):\n",
    "    \n",
    "    # Instantiate the model function; x is the input (feature) variable \n",
    "    model = model_func(x)\n",
    "    \n",
    "    # Instantiate the loss and error function\n",
    "    global y\n",
    "    loss, label_error = create_criterion_function_preferred(model, y)\n",
    "\n",
    "    # training config\n",
    "    epoch_size = 18000        # 18000 samples is half the dataset size \n",
    "    minibatch_size = 70\n",
    "    \n",
    "    # LR schedule over epochs \n",
    "    # In CNTK, an epoch is how often we get out of the minibatch loop to\n",
    "    # do other stuff (e.g. checkpointing, adjust learning rate, etc.)\n",
    "    # (we don't run this many epochs, but if we did, these are good values)\n",
    "    lr_per_sample = [0.003]*4+[0.0015]*24+[0.0003]\n",
    "    lr_per_minibatch = [lr * minibatch_size for lr in lr_per_sample]\n",
    "    lr_schedule = C.learning_rate_schedule(lr_per_minibatch, C.UnitType.minibatch, epoch_size)\n",
    "    \n",
    "    # Momentum schedule\n",
    "    momentum_as_time_constant = C.momentum_as_time_constant_schedule(700)\n",
    "    \n",
    "    # We use a the Adam optimizer which is known to work well on this dataset\n",
    "    # Feel free to try other optimizers from \n",
    "    # https://www.cntk.ai/pythondocs/cntk.learner.html#module-cntk.learner\n",
    "    learner = C.adam(parameters=model.parameters,\n",
    "                     lr=lr_schedule,\n",
    "                     momentum=momentum_as_time_constant,\n",
    "                     gradient_clipping_threshold_per_sample=15, \n",
    "                     gradient_clipping_with_truncation=True)\n",
    "\n",
    "    # Setup the progress updater\n",
    "    progress_printer = C.logging.ProgressPrinter(tag='Training', num_epochs=max_epochs)\n",
    "    \n",
    "    # Uncomment below for more detailed logging\n",
    "    #progress_printer = ProgressPrinter(freq=100, first=10, tag='Training', num_epochs=max_epochs) \n",
    "\n",
    "    # Instantiate the trainer\n",
    "    trainer = C.Trainer(model, (loss, label_error), learner, progress_printer)\n",
    "\n",
    "    # process minibatches and perform model training\n",
    "    C.logging.log_number_of_parameters(model)\n",
    "\n",
    "    t = 0\n",
    "    for epoch in range(max_epochs):         # loop over epochs\n",
    "        epoch_end = (epoch+1) * epoch_size\n",
    "        while t < epoch_end:                # loop over minibatches on the epoch\n",
    "            data = train_reader.next_minibatch(minibatch_size, input_map={  # fetch minibatch\n",
    "                x: train_reader.streams.query,\n",
    "                y: train_reader.streams[stream_name]\n",
    "            })\n",
    "            trainer.train_minibatch(data)               # update model with it\n",
    "            t += data[y].num_samples                    # samples so far\n",
    "        trainer.summarize_training_progress()\n",
    "    \n",
    "    while True:\n",
    "        minibatch_size = 500\n",
    "        data = test_reader.next_minibatch(minibatch_size, input_map={  # fetch minibatch\n",
    "            x: test_reader.streams.query,\n",
    "            y: test_reader.streams[stream_name]\n",
    "        })\n",
    "        if not data:                                 # until we hit the end\n",
    "            break\n",
    "        trainer.test_minibatch(data)\n",
    "    \n",
    "    trainer.summarize_test_progress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_train_test(stream_name, model_name):\n",
    "    models[model_name] = create_model()\n",
    "    train_reader = create_reader(data['train']['file'], is_training=True)\n",
    "    test_reader = create_reader(data['test']['file'], is_training=False)\n",
    "    train_test(train_reader, test_reader, models[model_name], stream_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 496218 parameters in 9 parameter tensors.\n",
      "Learning rate per minibatch: 0.21\n",
      "Finished Epoch[1 of 10]: [Training] loss = 0.135982 * 18003, metric = 3.42% * 18003 4.511s (3990.9 samples/s);\n",
      "Finished Epoch[2 of 10]: [Training] loss = 0.000101 * 18043, metric = 0.00% * 18043 3.485s (5177.3 samples/s);\n",
      "Finished Epoch[3 of 10]: [Training] loss = 0.000049 * 17968, metric = 0.00% * 17968 3.048s (5895.0 samples/s);\n",
      "Finished Epoch[4 of 10]: [Training] loss = 0.000030 * 18003, metric = 0.00% * 18003 3.207s (5613.7 samples/s);\n",
      "Learning rate per minibatch: 0.105\n",
      "Finished Epoch[5 of 10]: [Training] loss = 0.000023 * 18011, metric = 0.00% * 18011 3.069s (5868.7 samples/s);\n",
      "Finished Epoch[6 of 10]: [Training] loss = 0.000019 * 17975, metric = 0.00% * 17975 3.230s (5565.0 samples/s);\n",
      "Finished Epoch[7 of 10]: [Training] loss = 0.000016 * 18052, metric = 0.00% * 18052 3.595s (5021.4 samples/s);\n",
      "Finished Epoch[8 of 10]: [Training] loss = 0.000014 * 18002, metric = 0.00% * 18002 3.201s (5623.9 samples/s);\n",
      "Finished Epoch[9 of 10]: [Training] loss = 0.000012 * 17995, metric = 0.00% * 17995 3.439s (5232.6 samples/s);\n",
      "Finished Epoch[10 of 10]: [Training] loss = 0.000011 * 17999, metric = 0.00% * 17999 3.411s (5276.8 samples/s);\n",
      "Finished Evaluation [1]: Minibatch[1-1]: metric = 0.00% * 116;\n"
     ]
    }
   ],
   "source": [
    "do_train_test(\"slot_labels\", \"slots_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS const aReallyBigNumber ; EOS [67, 269, 130, 20, 80]\n",
      "(5, 18)\n",
      "[ 0  7  0 17  0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('BOS', 'O'),\n",
       " ('const', 'declaration.variable'),\n",
       " ('aReallyBigNumber', 'O'),\n",
       " (';', 'statement_end'),\n",
       " ('EOS', 'O')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dictionaries\n",
    "query_dict = {query_wl[i]:i for i in range(len(query_wl))}\n",
    "slots_dict = {slots_wl[i]:i for i in range(len(slots_wl))}\n",
    "intent_dict = {intent_wl[i]:i for i in range(len(intent_wl))}\n",
    "undefined_word = peer_words_wl[0]\n",
    "\n",
    "# let's run a sequence through\n",
    "#seq = \"BOS let i = 10000 ; EOS\"\n",
    "#seq = \"BOS isNaN ( 100 ) ; EOS\"\n",
    "#seq = \"BOS Array . isArray ( { pin , id , i , key , name } ) ; EOS\"\n",
    "#seq = \"BOS const obj = { a : 1 , b : 20000 , c : 1 , d : 2 } ; EOS\"\n",
    "#seq = \"BOS const arr = [ a , 1 , b , 20000 , c , 1 , d , 2 ] ; EOS\"\n",
    "seq = \"BOS const aReallyBigNumber ; EOS\"\n",
    "w = [query_dict[w] if w in query_dict else query_dict[undefined_word]\n",
    "     for w in seq.split()] # convert to word indices\n",
    "print(seq, w)\n",
    "onehot = np.zeros([len(w),len(query_dict)], np.float32)\n",
    "for t in range(len(w)):\n",
    "    onehot[t,w[t]] = 1\n",
    "\n",
    "#x = C.sequence.input_variable(vocab_size)\n",
    "pred = models['slots_model'](x).eval({x:[onehot]})[0]\n",
    "print(pred.shape)\n",
    "best = np.argmax(pred,axis=1)\n",
    "print(best)\n",
    "list(zip(seq.split(),[slots_wl[s] for s in best]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = C.sequence.input_variable(vocab_size)\n",
    "y = C.input_variable(num_intents)\n",
    "\n",
    "def create_model():\n",
    "    with C.layers.default_options(initial_state=0.1):\n",
    "        return C.layers.Sequential([\n",
    "            C.layers.Embedding(emb_dim, name='embed'),\n",
    "            C.layers.Stabilizer(),\n",
    "            C.layers.Fold(C.layers.LSTM(hidden_dim), go_backwards=False),\n",
    "            C.layers.Dense(num_intents, name='classify')\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 671704 parameters in 7 parameter tensors.\n",
      "Learning rate per minibatch: 0.21\n",
      "Finished Epoch[1 of 10]: [Training] loss = 0.009904 * 18004, metric = 0.33% * 18004 69.347s (259.6 samples/s);\n",
      "Finished Epoch[2 of 10]: [Training] loss = 0.000000 * 18001, metric = 0.00% * 18001 77.007s (233.8 samples/s);\n",
      "Finished Epoch[3 of 10]: [Training] loss = 0.000000 * 17999, metric = 0.00% * 17999 76.633s (234.9 samples/s);\n",
      "Finished Epoch[4 of 10]: [Training] loss = 0.000000 * 17999, metric = 0.00% * 17999 76.926s (234.0 samples/s);\n",
      "Learning rate per minibatch: 0.105\n",
      "Finished Epoch[5 of 10]: [Training] loss = 0.000000 * 17999, metric = 0.00% * 17999 77.168s (233.2 samples/s);\n",
      "Finished Epoch[6 of 10]: [Training] loss = 0.000000 * 18001, metric = 0.00% * 18001 77.494s (232.3 samples/s);\n",
      "Finished Epoch[7 of 10]: [Training] loss = 0.000000 * 18001, metric = 0.00% * 18001 78.229s (230.1 samples/s);\n",
      "Finished Epoch[8 of 10]: [Training] loss = 0.000000 * 18000, metric = 0.00% * 18000 78.263s (230.0 samples/s);\n",
      "Finished Epoch[9 of 10]: [Training] loss = 0.000000 * 17996, metric = 0.00% * 17996 77.981s (230.8 samples/s);\n",
      "Finished Epoch[10 of 10]: [Training] loss = 0.000000 * 18004, metric = 0.00% * 18004 77.771s (231.5 samples/s);\n",
      "Finished Evaluation [1]: Minibatch[1-1]: metric = 0.00% * 11;\n"
     ]
    }
   ],
   "source": [
    "do_train_test(\"intent\", \"intent_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS var tmp = arr [ i ] ; EOS : initialization\n"
     ]
    }
   ],
   "source": [
    "# let's run a sequence through\n",
    "#seq = \"BOS const aReallyBigNumber = ( 100 - 1 ) * 100 ; EOS\"\n",
    "seq = \"BOS var tmp = arr [ i ] ; EOS\"\n",
    "w = [query_dict[w] if w in query_dict else query_dict[undefined_word]\n",
    "     for w in seq.split()] # convert to word indices\n",
    "onehot = np.zeros([len(w),len(query_dict)], np.float32)\n",
    "for t in range(len(w)):\n",
    "    onehot[t,w[t]] = 1\n",
    "\n",
    "pred = models[\"intent_model\"](x).eval({x:[onehot]})[0]\n",
    "best = np.argmax(pred)\n",
    "print(seq, \":\", intent_wl[best])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
